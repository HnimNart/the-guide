# Exercises - do at least 1

These are only for levels 3 and 4

* Implement a version of the linear layer functions which uses shared memory and tiling
* Implement the tree reduction version of the sum function and add it to the softmax function.
Also compare the single pass and the tree reduction performance graphs. [Reference](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
* Implement a max pooling operator in all levels and implement tests
* Implement a convolution operator in all levels and implement tests
* Add reusable buffers to the computational graph system
* Extend the computational graph with inplace operation for the ReLU operator
