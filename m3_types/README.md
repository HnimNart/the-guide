# Types
## Types, energy usage and inference, quantization, sparsity and pruning of neural networks

* Floats
* Float precision
* Integers
* Energy usage (Vivienne Sze and the controversial paper)
* Inference
* Quantization
* Sparsity
* Pruning
* \*Fast inverse square root
* \*Bit tricks
* \*Basic compression
* \*Batch based data processing
* \*Tensor Cores
* \*Using integers instead of strings in hash tables

## \*Exercise

Find a suitable model and inference library.  
Perform inference.  
Optimize the model and inference process.  
Can you do inferencing on one thread,  
training on another and swap in the new model?  
ADD SUGGESTED MODELS  

## Additional Reading
[Full-Stack, GPU-based Acceleration of Deep Learning](https://nvlabs.github.io/EfficientDL/)
