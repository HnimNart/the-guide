# 2Ô∏è‚É£ Floats
Where as in signed integers we had two distinct areas of the bits with a uniform precision, we
have 3 segments and a quite varying precision with floating point numbers. In the more-or-less standard
IEEE-754 specification we also have a number of exceptions and specific behaviors such as in which situations
a float locks to a NaN value. Given the use cases in machine learning new, smaller and less standard floating point
formats have emerged such as bfloat16. A lot of these very small specialized types of floats, or very small integers
are required to work with tensor core accelerators.

I can't explain exactly how a float works much better than the
[wiki](https://en.wikipedia.org/wiki/Floating-point_arithmetic). Ever so slightly browsing the page about
[IEEE-754](https://en.wikipedia.org/wiki/IEEE_754) also makes good sense.

So, now that you've browsed the websites and your are familliar with concepts such as exponent and fraction, I
have a few key concepts for you to make note of. If you can at all keep your floats as close to being between
-1.0 and 1.0, you will keep as much of your precision as possible. 4.0 / 3.0 is not the same in terms of precision
as 4.0 * (1.0 / 3.0), unless the inverse number that you are multiplying with is exactly 1/2^N. This is an often
used optimization as a multiplication is vastly cheaper than a division. You will typically see this optimization
if a division by a constant is happening in a loop -

```rust
let some_constant: f32 = 5.0;
for element in &mut data {
    *element = *element / some_constant;
}

```

is not numerically equivalent to

```rust
let some_constant: f32 = 5.0;
let inverse_some_constant: f32 = 1.0 / some_constant;
for element in &mut data {
    *element = *element * inverse_some_constant;
}

```

But if ```some_constant``` was 4.0, 2.0 or 256.0 or something like that, they would be. Finally, NaN's propagate.
Any operation involving a NaN value returns a NaN value, including ```NaN == NaN```.

## 5Ô∏è‚É£ Additional Reading
Every floating point operation incurs some form of error. But if you used specialized floating point operations
such as the [fused multiply-add](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation), which
can be found [in Rust as well](https://doc.rust-lang.org/std/primitive.f32.html#method.mul_add), you can get a
single rounding error instead of two rounding errors. If you are summing a large list of numbers you can use
algorithms for compensating for the accumulated error, such as
[Kahan Summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). You can also keep a
[running estimate of the error](https://pbr-book.org/3ed-2018/Shapes/Managing_Rounding_Error).

### üß¨ Computer Graphics
Depth buffers are usually in need of some thought. You can take a look at
[depth precision visualized](https://developer.nvidia.com/content/depth-precision-visualized) or the now
fairly common [reverse depth buffer](https://www.danielecarbone.com/reverse-depth-buffer-in-opengl/).
